# Testing Guide - Backend Microservices

Este documento describe c√≥mo ejecutar y trabajar con los tests unitarios de los microservicios.

## üìã Descripci√≥n General

Los tests unitarios est√°n dise√±ados para ejecutarse **sin necesidad de levantar las APIs** ni conectarse a bases de datos reales. Utilizan:

- **SQLite en memoria** para simulaci√≥n de base de datos
- **Mocks** para servicios externos (HTTP, audit client)
- **Fixtures** para datos de prueba consistentes
- **pytest** como framework principal
- **Coverage** para reportes de cobertura

## üèóÔ∏è Estructura de Testing

### User Service Tests
```
user-service/
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_user_service.py    # Tests del servicio principal
‚îÇ   ‚îî‚îÄ‚îÄ test_models.py          # Tests de modelos Pydantic
‚îú‚îÄ‚îÄ conftest.py                 # Configuraci√≥n y fixtures
‚îú‚îÄ‚îÄ pytest.ini                 # Configuraci√≥n de pytest
‚îú‚îÄ‚îÄ requirements-test.txt       # Dependencias de testing
‚îú‚îÄ‚îÄ run_tests.ps1              # Script PowerShell
‚îî‚îÄ‚îÄ run_tests.sh               # Script Bash
```

### Audit Service Tests
```
audit-service/
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_audit_service.py   # Tests del servicio principal
‚îÇ   ‚îî‚îÄ‚îÄ test_audit_models.py    # Tests de modelos Pydantic
‚îú‚îÄ‚îÄ conftest.py                 # Configuraci√≥n y fixtures
‚îú‚îÄ‚îÄ pytest.ini                 # Configuraci√≥n de pytest
‚îú‚îÄ‚îÄ requirements-test.txt       # Dependencias de testing
‚îî‚îÄ‚îÄ run_tests.ps1              # Script PowerShell
```

## üöÄ Ejecuci√≥n de Tests

### Opci√≥n 1: Scripts Autom√°ticos (Recomendado)

#### Windows PowerShell:
```powershell
# User Service
cd user-service
.\run_tests.ps1

# Audit Service  
cd audit-service
.\run_tests.ps1
```

#### Linux/Mac:
```bash
# User Service
cd user-service
chmod +x run_tests.sh
./run_tests.sh

# Audit Service
cd audit-service
chmod +x run_tests.sh  
./run_tests.sh
```

### Opci√≥n 2: Comandos Manuales

```bash
# 1. Instalar dependencias
pip install -r requirements-test.txt

# 2. Ejecutar todos los tests
pytest tests/ -v

# 3. Ejecutar tests espec√≠ficos
pytest tests/test_user_service.py::TestUserServicePasswordValidation -v

# 4. Ejecutar con coverage
pip install pytest-cov
pytest tests/ --cov=app --cov-report=html --cov-report=term

# 5. Ejecutar solo tests marcados
pytest tests/ -m "unit" -v
```

## üìä Cobertura de Tests

### User Service (24 tests)

#### `TestUserServicePasswordValidation` (5 tests)
- ‚úÖ Contrase√±as v√°lidas con todos los requisitos
- ‚úÖ Rechazo de contrase√±as muy cortas
- ‚úÖ Rechazo sin may√∫sculas/min√∫sculas/n√∫meros/especiales

#### `TestUserServiceNITValidation` (4 tests)  
- ‚úÖ Validaci√≥n exitosa de NIT existente
- ‚úÖ Manejo de NIT no encontrado (404)
- ‚úÖ Rechazo de instituciones inactivas
- ‚úÖ Manejo de errores de red

#### `TestUserServiceJWTGeneration` (2 tests)
- ‚úÖ Generaci√≥n de tokens v√°lidos
- ‚úÖ Inclusi√≥n de datos de usuario en token

#### `TestUserServiceUserCreation` (4 tests)
- ‚úÖ Creaci√≥n exitosa completa con auditor√≠a
- ‚úÖ Rechazo de contrase√±as d√©biles (422)
- ‚úÖ Rechazo de NITs no autorizados (404)
- ‚úÖ Rechazo de emails duplicados (409)

#### `TestUserServiceHashingUtilities` (2 tests)
- ‚úÖ Generaci√≥n de hashes bcrypt v√°lidos
- ‚úÖ Hashes √∫nicos por llamada (sales)

#### `TestUserRegisterModel` (4 tests)
- ‚úÖ Validaci√≥n de modelos Pydantic
- ‚úÖ Rechazo de emails inv√°lidos
- ‚úÖ Validaci√≥n de campos obligatorios

### Audit Service (20 tests)

#### `TestAuditServiceCreate` (5 tests)
- ‚úÖ Creaci√≥n exitosa de logs de auditor√≠a
- ‚úÖ Generaci√≥n de IDs √∫nicos autom√°ticos
- ‚úÖ Logs para eventos fallidos
- ‚úÖ Preservaci√≥n de timestamps
- ‚úÖ Creaci√≥n de m√∫ltiples logs

#### `TestAuditServiceRetrieval` (3 tests)
- ‚úÖ Recuperaci√≥n de logs existentes
- ‚úÖ Manejo de logs inexistentes
- ‚úÖ Recuperaci√≥n post-creaci√≥n

#### `TestAuditServiceDataIntegrity` (2 tests)
- ‚úÖ Preservaci√≥n de estructura de requests
- ‚úÖ Almacenamiento correcto de enums

#### `TestAuditServiceErrorHandling` (2 tests)
- ‚úÖ Manejo de sesi√≥n de base de datos
- ‚úÖ Creaci√≥n con datos m√≠nimos

#### `TestModelos y Enums` (8 tests)
- ‚úÖ Validaci√≥n de RequestData, AuditLogCreate, AuditLogResponse
- ‚úÖ Validaci√≥n de enums OutcomeType y ActionType
- ‚úÖ Tests de integraci√≥n entre modelos

## üîß Configuraci√≥n Avanzada

### Ejecutar Tests con Filtros
```bash
# Solo tests de validaci√≥n de password
pytest -k "password" -v

# Solo tests que no requieren red
pytest -k "not network" -v

# Tests r√°pidos solamente
pytest -m "not slow" -v
```

### Coverage Detallado
```bash
# Coverage con exclusi√≥n de l√≠neas
pytest --cov=app --cov-report=html --cov-fail-under=90

# Coverage solo para archivos modificados
pytest --cov=app --cov-report=term-missing
```

### Debug de Tests
```bash
# Parar en primer fallo
pytest -x

# Verbose con stdout
pytest -v -s

# Debugger en fallos
pytest --pdb
```

## üêõ Soluci√≥n de Problemas

### Error: "No module named 'pytest'"
```bash
pip install -r requirements-test.txt
```

### Error: "cannot import name 'app'"
```bash
# Aseg√∫rate de estar en el directorio correcto
cd user-service  # o audit-service
export PYTHONPATH=$PWD:$PYTHONPATH
```

### Tests Muy Lentos
```bash
# Ejecutar en paralelo
pip install pytest-xdist
pytest -n 4 tests/
```

### Error de Base de Datos
Los tests usan SQLite en memoria, no requieren PostgreSQL. Si hay errores:
```bash
# Limpiar cache de pytest
pytest --cache-clear
```

## üìà M√©tricas de Calidad

### Objetivos de Coverage
- **User Service**: >90% cobertura
- **Audit Service**: >95% cobertura
- **Modelos**: 100% cobertura

### Tiempo de Ejecuci√≥n Objetivo
- **Tests unitarios**: <30 segundos por servicio
- **Tests completos**: <1 minuto total

### Est√°ndares de C√≥digo
- Todos los tests deben pasar en CI/CD
- No warnings de deprecaci√≥n
- Documentaci√≥n en docstrings